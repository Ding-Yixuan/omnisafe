# import numpy as np
# import safety_gymnasium
# import torch
# import os

# # 1. åŠ è½½è®­ç»ƒæ•°æ®
# DATA_PATH = './datasets/dataset_raw.npz' # ç¡®ä¿è·¯å¾„å¯¹
# print(f"æ­£åœ¨è¯»å–è®­ç»ƒæ•°æ®: {DATA_PATH}")
# data = np.load(DATA_PATH)
# train_obs = data['observations']

# # è®¡ç®—è®­ç»ƒæ•°æ®çš„æ¯ä¸€ç»´çš„å‡å€¼å’Œæ–¹å·®
# train_mean = np.mean(train_obs, axis=0)
# train_std = np.std(train_obs, axis=0)
# print(f"è®­ç»ƒæ•°æ® Obs ç»´åº¦: {train_obs.shape}")

# # 2. åˆ›å»ºè¯„ä¼°ç¯å¢ƒ (ä½¿ç”¨ä½ çš„ Patch é€»è¾‘)
# from safety_gymnasium.tasks.safe_navigation.goal.goal_level1 import GoalLevel1
# from safety_gymnasium.tasks.safe_navigation.goal.goal_level0 import GoalLevel0
# from safety_gymnasium.assets.geoms import Hazards
# import gymnasium

# # --- å¤åˆ¶ä½ çš„ Patch ä»£ç  ---
# def patched_init(self, config):
#     self.lidar_num_bins = 16
#     self.lidar_max_dist = 3.0
#     self.sensors_obs = ['accelerometer', 'velocimeter', 'gyro', 'magnetometer']
#     self.task_name = 'GoalLevel1_Reproduction'
#     config.update({'lidar_num_bins': 16, 'lidar_max_dist': 3.0, 
#                    'sensors_obs': self.sensors_obs, 'task_name': self.task_name})
#     GoalLevel0.__init__(self, config=config)
#     self.placements_conf.extents = [-1.5, -1.5, 1.5, 1.5]
#     self._add_geoms(Hazards(num=2, keepout=0.18))

# def patched_obs(self):
#     # ä½ çš„æ‰‹åŠ¨æ‹¼æ¥é€»è¾‘
#     lidar_vec = self._obs_lidar(self.hazards.pos, self.hazards.group) 
#     acc = self.agent.get_sensor('accelerometer')[:2]
#     vel = self.agent.get_sensor('velocimeter')[:2]
#     gyro = self.agent.get_sensor('gyro')[-1:]
#     mag = self.agent.get_sensor('magnetometer')[:2]
#     sensor_vec = np.concatenate([acc, vel, gyro, mag])
    
#     vec = (self.goal.pos - self.agent.pos) @ self.agent.mat
#     x, y = vec[0], vec[1]
#     z = x + 1j * y
#     dist = np.exp(-np.abs(z)) 
#     angle = np.angle(z)
#     goal_vec = np.array([dist, np.cos(angle), np.sin(angle)])
    
#     return np.concatenate([sensor_vec, goal_vec, lidar_vec]).astype(np.float32)

# GoalLevel1.__init__ = patched_init
# GoalLevel1.obs = patched_obs
# # -------------------------

# print("æ­£åœ¨åˆå§‹åŒ–è¯„ä¼°ç¯å¢ƒ...")
# env = safety_gymnasium.make('SafetyPointGoal1-v0', render_mode='rgb_array', camera_name='fixedfar', width=256, height=256)

# # 3. é‡‡æ ·ä¸€äº›ç¯å¢ƒæ•°æ®
# env_obs_list = []
# obs, _ = env.reset()
# for _ in range(1000):
#     action = env.action_space.sample()
#     obs, _, _, _, _, _ = env.step(action)
#     env_obs_list.append(obs)

# env_obs = np.array(env_obs_list)
# env_mean = np.mean(env_obs, axis=0)
# env_std = np.std(env_obs, axis=0)

# # 4. å¯¹æ¯”æ‰“å°
# print("\n" + "="*60)
# print(f"{'Dim':<5} | {'Train Mean':<12} | {'Eval Mean':<12} | {'Diff':<10} | {'åˆ¤æ–­'}")
# print("-" * 60)

# mismatch_count = 0
# for i in range(26):
#     t_m = train_mean[i]
#     e_m = env_mean[i]
#     diff = abs(t_m - e_m)
    
#     # ç®€å•çš„å¯å‘å¼åˆ¤æ–­
#     status = "âœ…"
#     if diff > 0.5: status = "âŒ åå·®å¤§"
#     if i >= 10 and i < 26: # é›·è¾¾åŒºåŸŸ
#         if t_m < 0.1 and e_m > 0.5: status = "â“ åªæœ‰ç¯å¢ƒæœ‰éšœç¢?"
#         if t_m > 0.5 and e_m < 0.1: status = "â“ åªæœ‰è®­ç»ƒæœ‰éšœç¢?"
        
#     print(f"{i:<5} | {t_m:12.4f} | {e_m:12.4f} | {diff:10.4f} | {status}")
#     if diff > 1.0: mismatch_count += 1

# print("="*60)
# if mismatch_count > 3:
#     print("ğŸ’€ ä¸¥é‡è­¦å‘Šï¼šè§‚æµ‹åˆ†å¸ƒå·®å¼‚å·¨å¤§ï¼ä½ çš„ patched_obs é¡ºåºå¾ˆå¯èƒ½å†™é”™äº†ï¼")
#     print("å»ºè®®ï¼šæ£€æŸ¥ patched_obs é‡Œçš„æ‹¼æ¥é¡ºåºï¼Œæ˜¯ä¸æ˜¯æŠŠ Goal å’Œ Lidar æåäº†ï¼Ÿ")
# else:
#     print("âœ¨ åˆ†å¸ƒçœ‹èµ·æ¥åŸºæœ¬ä¸€è‡´ï¼Œè§‚æµ‹é¡ºåºåº”è¯¥æ²¡é—®é¢˜ã€‚é—®é¢˜å¯èƒ½åœ¨ Action å½’ä¸€åŒ–ã€‚")

# import numpy as np
# import os

# # æŒ‡å‘ä½ æ­£åœ¨ç”¨çš„é‚£ä¸ªæ–‡ä»¶
# DATA_PATH = '/home/lqz27/dyx_ws/omnisafe/datasets/dataset_raw.npz'

# print(f"æ­£åœ¨æ£€æŸ¥æ–‡ä»¶: {DATA_PATH}")

# if not os.path.exists(DATA_PATH):
#     print("âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼è·¯å¾„é”™äº†å—ï¼Ÿ")
# else:
#     data = np.load(DATA_PATH)
#     actions = data['actions']
#     obs = data['observations']
    
#     print(f"æ•°æ®é‡ (Steps): {actions.shape[0]}")
    
#     # 1. æ£€æŸ¥åŠ¨ä½œæ–¹å·®
#     act_std = np.std(actions, axis=0)
#     print(f"ğŸ“Š åŠ¨ä½œæ–¹å·® (Action Std): {act_std}")
    
#     if np.mean(act_std) < 0.1:
#         print("ğŸš¨ ã€å®é”¤äº†ã€‘è¿™æ˜¯æ—§æ•°æ®ï¼æ–¹å·®æä½ï¼Œæœºå™¨äººåœ¨ç”»åœ†æˆ–ä¸åŠ¨ã€‚")
#         print("   -> è¯·æ£€æŸ¥ preprocess è„šæœ¬æ˜¯å¦çœŸçš„æŠŠ v2 æ•°æ®å†™è¿›å»äº†ã€‚")
#     else:
#         print("âœ… åŠ¨ä½œæ–¹å·®æ­£å¸¸ï¼Œç¡®å®æ˜¯æ–°æ•°æ®ã€‚")

#     # 2. æ£€æŸ¥è§‚æµ‹æ–¹å·® (å¯¼è‡´ 5.000 çˆ†ç‚¸çš„åŸå› )
#     obs_std = np.std(obs, axis=0)
#     print(f"ğŸ“Š è§‚æµ‹æ–¹å·® (Obs Std Min/Max): {np.min(obs_std):.6f} / {np.max(obs_std):.6f}")
    
#     # æ£€æŸ¥æ˜¯å¦æœ‰æå°æ–¹å·®
#     low_var_dims = np.where(obs_std < 1e-4)[0]
#     if len(low_var_dims) > 0:
#         print(f"âš ï¸ è­¦å‘Š: ç¬¬ {low_var_dims} ç»´åº¦çš„æ–¹å·®æå°ï¼")
#         print("   è¿™æ„å‘³ç€è®­ç»ƒé›†é‡Œè¿™äº›ç»´åº¦å‡ ä¹æ²¡å˜è¿‡ï¼Œä½†åœ¨æµ‹è¯•æ—¶ä¸€å˜å°±ä¼šå¯¼è‡´å½’ä¸€åŒ–çˆ†ç‚¸ã€‚")

# import numpy as np
# # æ¢æˆä½ çš„ dataset_v2_raw è·¯å¾„
# data = np.load('/home/lqz27/dyx_ws/omnisafe/datasets/dataset_raw.npz') 
# actions = data['actions']

# print(f"åŠ¨ä½œå‡å€¼: {np.mean(actions, axis=0)}")
# # å¦‚æœ Action[1] (ç¬¬äºŒé¡¹) æ˜¯è´Ÿæ•°ï¼ˆæ¯”å¦‚ -0.4ï¼‰ï¼Œè¯´æ˜ä¸“å®¶æœ¬èº«å°±å–œæ¬¢å³è½¬ã€‚
# # å¦‚æœ Action[1] æ¥è¿‘ 0 (æ¯”å¦‚ -0.05)ï¼Œè¯´æ˜æ•°æ®æ²¡é—®é¢˜ï¼Œæ˜¯æ¨¡å‹è¿˜æ²¡ç»ƒå¥½ã€‚

import os
import torch
import numpy as np
import omnisafe
import safety_gymnasium
import gymnasium
from safety_gymnasium.assets.geoms import Hazards
from safety_gymnasium.tasks.safe_navigation.goal.goal_level1 import GoalLevel1
from safety_gymnasium.tasks.safe_navigation.goal.goal_level0 import GoalLevel0

# =================================================================
# 1. Monkey Patch (å¿…é¡»åŠ ï¼Œå› ä¸º checkpoint æ˜¯ 26 ç»´çš„)
# =================================================================
def patched_init(self, config):
    self.lidar_num_bins = 16
    self.lidar_max_dist = 3.0
    self.sensors_obs = ['accelerometer', 'velocimeter', 'gyro', 'magnetometer']
    self.task_name = 'GoalLevel1_Reproduction'
    config.update({'lidar_num_bins': 16, 'lidar_max_dist': 3.0, 'sensors_obs': self.sensors_obs, 'task_name': self.task_name})
    GoalLevel0.__init__(self, config=config)
    self.placements_conf.extents = [-1.5, -1.5, 1.5, 1.5]
    self._add_geoms(Hazards(num=2, keepout=0.18))

def patched_build_observation_space(self):
    self.observation_space = gymnasium.spaces.Box(low=-np.inf, high=np.inf, shape=(26,), dtype=np.float32)

def patched_obs(self):
    lidar_vec = self._obs_lidar(self.hazards.pos, self.hazards.group) 
    acc = self.agent.get_sensor('accelerometer')[:2]
    vel = self.agent.get_sensor('velocimeter')[:2]
    gyro = self.agent.get_sensor('gyro')[-1:]
    mag = self.agent.get_sensor('magnetometer')[:2]
    sensor_vec = np.concatenate([acc, vel, gyro, mag])
    vec = (self.goal.pos - self.agent.pos) @ self.agent.mat
    x, y = vec[0], vec[1]
    z = x + 1j * y
    dist = np.exp(-np.abs(z)) 
    angle = np.angle(z)
    goal_vec = np.array([dist, np.cos(angle), np.sin(angle)])
    return np.concatenate([sensor_vec, goal_vec, lidar_vec]).astype(np.float32)

GoalLevel1.__init__ = patched_init
GoalLevel1.build_observation_space = patched_build_observation_space
GoalLevel1.obs = patched_obs
print("âœ… ç¯å¢ƒ Patch å·²åº”ç”¨ (26ç»´)")

# ================= é…ç½® =================
LOG_DIR = './runs/PPOLag-{SafetyPointGoal1-v0}/seed-000-2026-02-09-14-10-04'

# ================= è¾…åŠ©å‡½æ•° =================
def find_actor(obj, depth=0):
    if depth > 4: return None
    if hasattr(obj, 'predict') and callable(getattr(obj, 'predict')):
        if not isinstance(obj, omnisafe.Evaluator): return obj
    for attr_name in dir(obj):
        if attr_name.startswith('__'): continue
        try:
            attr_obj = getattr(obj, attr_name)
            res = find_actor(attr_obj, depth + 1)
            if res: return res
        except: continue
    return None

def verify_ppo_performance():
    print(f"ğŸ” æ­£åœ¨åŠ è½½ PPO æ¨¡å‹...")
    evaluator = omnisafe.Evaluator()
    
    try:
        evaluator.load_saved(save_dir=LOG_DIR, model_name='epoch-100.pt')
        print("âœ… åŠ è½½äº† epoch-500.pt")
    except:
        try:
            evaluator.load_saved(save_dir=LOG_DIR, model_name='model.pt')
            print("âœ… åŠ è½½äº† model.pt")
        except:
            print("âŒ åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
            return

    agent = find_actor(evaluator)
    if agent is None:
        print("âŒ æ— æ³•æ‰¾åˆ°ç­–ç•¥ç½‘ç»œï¼")
        return
        
    env = evaluator._env
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    if hasattr(agent, 'to'): agent.to(device)

    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Env Dim: {env.observation_space.shape}")
    
    # è·‘ 5 ä¸ª Episode çœ‹çœ‹å¹³å‡åˆ†
    num_episodes = 5
    total_rewards = []
    
    for i in range(num_episodes):
        obs, _ = env.reset()
        done = False
        step = 0
        ep_ret = 0
        ep_cost = 0
        
        first_actions = []
        
        while not done and step < 1000:
            with torch.no_grad():
                # ç¡®ä¿ obs æ˜¯ Tensor (Batch=1)
                if isinstance(obs, np.ndarray):
                    obs_tensor = torch.as_tensor(obs, dtype=torch.float32).to(device).unsqueeze(0)
                else:
                    obs_tensor = obs.to(device).unsqueeze(0)
                
                # é¢„æµ‹åŠ¨ä½œ (Tensor on GPU) -> Shapeé€šå¸¸æ˜¯ (1, 2)
                act = agent.predict(obs_tensor, deterministic=True)
                
                if step < 10: 
                    first_actions.append(act.squeeze(0).cpu().numpy())

            # ğŸ”¥ã€ä¿®å¤å…³é”®ç‚¹ã€‘
            # 1. æŠŠåŠ¨ä½œæ¬å› CPU
            # 2. å‹ç¼©ç»´åº¦ (1, 2) -> (2,)
            act_cpu_tensor = act.cpu().squeeze(0) 
            
            # è¯Šæ–­ï¼šå¦‚æœæ˜¯ç¬¬ä¸€æ­¥ï¼Œæ‰“å°ä¸€ä¸‹å½¢çŠ¶ç¡®è®¤
            if step == 0 and i == 0:
                print(f"ğŸ” åŠ¨ä½œå½¢çŠ¶æ£€æŸ¥: Original={act.shape}, Squeezed={act_cpu_tensor.shape}")
            
            res = env.step(act_cpu_tensor)
            
            if len(res) == 6:
                obs, reward, cost, terminated, truncated, _ = res
            elif len(res) == 5:
                obs, reward, cost, terminated, truncated = res
            
            # å¤„ç†è¿”å›å€¼ç±»å‹
            if isinstance(reward, torch.Tensor): reward = reward.item()
            if isinstance(cost, torch.Tensor): cost = cost.item()
            if isinstance(terminated, torch.Tensor): terminated = bool(terminated.item())
            if isinstance(truncated, torch.Tensor): truncated = bool(truncated.item())
            
            ep_ret += reward
            ep_cost += cost
            step += 1
            
            if terminated or truncated:
                done = True
        
        total_rewards.append(ep_ret)
        
        avg_act = np.mean(first_actions, axis=0)
        print(f"Episode {i+1}: Reward={ep_ret:.2f}, Cost={ep_cost}, Steps={step}")
        print(f"   å¼€å±€å¹³å‡åŠ¨ä½œ: {avg_act}")
        if avg_act[1] < -0.1 or avg_act[1] > 0.1:
            print("   âš ï¸  è­¦å‘Š: æ˜æ˜¾åœ¨è½¬åœˆï¼")

    print(f"="*30)
    avg_score = np.mean(total_rewards)
    print(f"ğŸ“Š å¹³å‡å¾—åˆ†: {avg_score:.2f}")
    
    if avg_score < 5.0:
        print("âŒ ç»“è®ºï¼šå®é”¤äº†ï¼PPO æ¨¡å‹æ ¹æœ¬æ²¡è®­ç»ƒå¥½ã€‚")
        print("   -> èµ¶ç´§é‡è®­ PPO å§ï¼Œåˆ«æµªè´¹æ—¶é—´äº†ã€‚")
    else:
        print("âœ… ç»“è®ºï¼šPPO æ˜¯å¥½çš„ï¼")
        print("   -> ä¹‹å‰çš„æ•°æ®é‡‡é›†è„šæœ¬æœ‰ Bugï¼Œæˆ‘ç»™ä½ æ–°çš„ã€‚")

if __name__ == '__main__':
    verify_ppo_performance()