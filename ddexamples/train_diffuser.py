import torch
import torch.nn as nn
import numpy as np
import os
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt

# =================================================================
# 1. é…ç½®å‚æ•°
# =================================================================
CONFIG = {
    'dataset_path': './data_pro/ppolag_256.npz',
    'horizon': 64,          # ä¸€æ¬¡ç”Ÿæˆ64æ­¥ (H)
    'obs_dim': 26,          # è§‚æµ‹ç»´åº¦
    'act_dim': 2,           # åŠ¨ä½œç»´åº¦
    'hidden_dim': 256,      # ç½‘ç»œéšè—å±‚ç»´åº¦
    'train_steps': 50000,  # è®­ç»ƒæ­¥æ•° Gradient Steps
    'batch_size': 256,     
    'lr': 2e-4,             
    'device': 'cuda:0',     
    'save_dir': './çœ‹lossæ›²çº¿/ppolag_256',
}

# =================================================================
# 2. æ•°æ®é›† (TrajectoryDataset)
# =================================================================
class TrajectoryDataset(Dataset):
    def __init__(self, data_path, horizon=64):
        print(f"Loading data from {data_path}...")
        raw_data = np.load(data_path)
        
        # æå–æ ¸å¿ƒæ•°æ®
        self.obs = raw_data['obs'].astype(np.float32)
        self.act = raw_data['act'].astype(np.float32)
        self.segment_ids = raw_data['segment_id']
        
        # 1. å½’ä¸€åŒ– (MinMax Normalization to [-1, 1])
        # Diffuser å¯¹æ•°æ®èŒƒå›´éå¸¸æ•æ„Ÿï¼Œå¿…é¡»å½’ä¸€åŒ–
        self.mins = np.concatenate([self.obs.min(axis=0), self.act.min(axis=0)])
        self.maxs = np.concatenate([self.obs.max(axis=0), self.act.max(axis=0)])
        
        # é˜²æ­¢åˆ†æ¯ä¸º0 (å¦‚æœæŸç»´æ•°æ®æ²¡å˜è¿‡)
        self.maxs[self.maxs == self.mins] += 1.0
        
        # 2. æ„å»ºç´¢å¼• (æ‰¾å‡ºæ‰€æœ‰åˆæ³•çš„åˆ‡ç‰‡èµ·ç‚¹)
        # æˆ‘ä»¬ä¸èƒ½è·¨è¶Š segment_idï¼Œä¹Ÿä¸èƒ½åˆ‡å‡ºé•¿åº¦å°äº horizon çš„ç‰‡æ®µ
        self.indices = []
        total_steps = len(self.obs)
        
        print("âœ‚ï¸  Slicing trajectories...")
        for i in range(total_steps - horizon + 1):
            # æ£€æŸ¥èµ·ç‚¹å’Œç»ˆç‚¹æ˜¯å¦åœ¨åŒä¸€ä¸ª Segment å†…
            if self.segment_ids[i] == self.segment_ids[i + horizon - 1]:
                self.indices.append(i)
                
        print(f"âœ… Created {len(self.indices)} sequences (Horizon={horizon})")
        
    def normalize(self, x):
        """ [0, 1] -> [-1, 1] """
        # å…ˆå½’ä¸€åŒ–åˆ° [0, 1]
        x_norm = (x - self.mins) / (self.maxs - self.mins)
        # å†æ˜ å°„åˆ° [-1, 1]
        return 2 * x_norm - 1

    def unnormalize(self, x):
        """ [-1, 1] -> original """
        x_01 = (x + 1) / 2
        return x_01 * (self.maxs - self.mins) + self.mins

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        start_t = self.indices[idx]
        end_t = start_t + CONFIG['horizon']
        
        # å–å‡ºè¿™ä¸€æ®µçš„ obs å’Œ act
        obs_seq = self.obs[start_t:end_t]
        act_seq = self.act[start_t:end_t]
        
        # æ‹¼æ¥æˆ Joint Trajectory: [H, obs_dim + act_dim]
        traj = np.concatenate([obs_seq, act_seq], axis=-1)
        
        # å½’ä¸€åŒ–
        traj_norm = self.normalize(traj)
        return torch.tensor(traj_norm, dtype=torch.float32)

# =================================================================
# 3. ç½‘ç»œæ¶æ„ (Temporal U-Net Block)
# =================================================================
class SinusoidalPosEmb(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        device = x.device
        half_dim = self.dim // 2
        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
        emb = x[:, None] * emb[None, :]
        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
        return emb

class TemporalUnet(nn.Module):
    def __init__(self, transition_dim, dim=128):
        super().__init__()
        
        # æ—¶é—´æ­¥ç¼–ç  (Time Embedding)
        self.time_mlp = nn.Sequential(
            SinusoidalPosEmb(dim),
            nn.Linear(dim, dim * 4),
            nn.Mish(),
            nn.Linear(dim * 4, dim),
        )

        # ç¼–ç å™¨ (Downsample)
        self.down1 = nn.Sequential(nn.Conv1d(transition_dim, dim, 3, padding=1), nn.Mish())
        self.down2 = nn.Sequential(nn.Conv1d(dim, dim * 2, 3, padding=1), nn.Mish())
        self.down3 = nn.Sequential(nn.Conv1d(dim * 2, dim * 4, 3, padding=1), nn.Mish())

        # è§£ç å™¨ (Upsample)
        self.up1 = nn.Sequential(nn.Conv1d(dim * 4, dim * 2, 3, padding=1), nn.Mish())
        self.up2 = nn.Sequential(nn.Conv1d(dim * 2, dim, 3, padding=1), nn.Mish())
        self.final_conv = nn.Conv1d(dim, transition_dim, 1)

    def forward(self, x, t):
        # x: [Batch, Dim, Horizon] (æ³¨æ„ Conv1d éœ€è¦ Dim åœ¨ä¸­é—´)
        # t: [Batch]
        
        t_emb = self.time_mlp(t).unsqueeze(-1) # [Batch, Dim, 1]
        
        # Down
        x1 = self.down1(x) + t_emb
        x2 = self.down2(x1)
        x3 = self.down3(x2)
        
        # Up (ç®€å•çš„ U-Net è¿æ¥)
        x = self.up1(x3) + x2
        x = self.up2(x) + x1
        x = self.final_conv(x)
        return x

# =================================================================
# 4. æ‰©æ•£è¿‡ç¨‹ç®¡ç† (GaussianDiffusion)
# =================================================================
class GaussianDiffusion(nn.Module):
    def __init__(self, model, horizon, transition_dim, n_timesteps=1000):
        super().__init__()
        self.model = model
        self.horizon = horizon
        self.transition_dim = transition_dim
        self.n_timesteps = n_timesteps

        # å®šä¹‰ Beta Schedule (Linear)
        betas = torch.linspace(1e-4, 2e-2, n_timesteps)
        alphas = 1. - betas
        alphas_cumprod = torch.cumprod(alphas, dim=0)
        
        # æ³¨å†Œ bufferï¼Œè¿™æ ·å®ƒä»¬ä¼šè‡ªåŠ¨è½¬åˆ° GPU
        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))
        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))

    def compute_loss(self, x_0):
        """ è®¡ç®—å»å™ª Loss """
        batch_size = x_0.shape[0]
        
        # 1. éšæœºé‡‡æ ·æ—¶é—´æ­¥ t
        t = torch.randint(0, self.n_timesteps, (batch_size,), device=x_0.device).long()
        
        # 2. ç”Ÿæˆå™ªå£° noise
        noise = torch.randn_like(x_0)
        
        # 3. åŠ å™ª (Forward Process): x_t = sqrt(alpha_bar) * x_0 + sqrt(1-alpha_bar) * noise
        # æå–å¯¹åº” t çš„ç³»æ•°
        coef1 = self.sqrt_alphas_cumprod[t].reshape(-1, 1, 1)
        coef2 = self.sqrt_one_minus_alphas_cumprod[t].reshape(-1, 1, 1)
        
        x_t = coef1 * x_0 + coef2 * noise
        
        # 4. æ¨¡å‹é¢„æµ‹å™ªå£° (Model Prediction)
        # Conv1d éœ€è¦ [Batch, Dim, Horizon]ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦ permute
        x_t_in = x_t.permute(0, 2, 1) 
        noise_pred = self.model(x_t_in, t)
        noise_pred = noise_pred.permute(0, 2, 1) # è½¬å› [Batch, Horizon, Dim]
        
        # 5. è®¡ç®— MSE Loss
        return nn.functional.mse_loss(noise_pred, noise)

# =================================================================
# 5. ä¸»è®­ç»ƒå¾ªç¯
# =================================================================
def train():
    os.makedirs(CONFIG['save_dir'], exist_ok=True)
    device = torch.device(CONFIG['device'])
    
    # 1. å‡†å¤‡æ•°æ®
    dataset = TrajectoryDataset(CONFIG['dataset_path'], horizon=CONFIG['horizon'])
    dataloader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4, pin_memory=True)
    
    # ä¿å­˜å½’ä¸€åŒ–å‚æ•°ï¼Œæ¨ç†æ—¶è¦ç”¨
    np.savez(os.path.join(CONFIG['save_dir'], 'normalization.npz'), mins=dataset.mins, maxs=dataset.maxs)
    print("âœ… Normalization params saved.")

    # 2. åˆå§‹åŒ–æ¨¡å‹
    transition_dim = CONFIG['obs_dim'] + CONFIG['act_dim']
    unet = TemporalUnet(transition_dim=transition_dim, dim=CONFIG['hidden_dim']).to(device)
    diffusion = GaussianDiffusion(unet, CONFIG['horizon'], transition_dim).to(device)
    
    optimizer = torch.optim.Adam(unet.parameters(), lr=CONFIG['lr'])
    
    print(f"ğŸš€ Start Training Diffuser... Steps: {CONFIG['train_steps']}")
    
    step = 0
    loss_history = []
    
    while step < CONFIG['train_steps']:
        for batch_traj in dataloader:
            batch_traj = batch_traj.to(device) # [Batch, Horizon, Dim]
            
            # è®¡ç®— Loss
            loss = diffusion.compute_loss(batch_traj)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            step += 1
            loss_history.append(loss.item())
            
            if step % 100 == 0:
                print(f"Step {step}/{CONFIG['train_steps']} | Loss: {loss.item():.6f}")
                
            if step % 5000 == 0:
                # ä¿å­˜æ¨¡å‹
                save_path = os.path.join(CONFIG['save_dir'], f'diffuser_step_{step}.pt')
                torch.save(unet.state_dict(), save_path)
                print(f"ğŸ’¾ Model saved to {save_path}")
                
                # ç”» Loss æ›²çº¿
                plt.figure()
                plt.plot(loss_history)
                plt.title("Diffusion Training Loss")
                plt.xlabel("Steps")
                plt.ylabel("MSE Loss")
                plt.savefig(os.path.join(CONFIG['save_dir'], 'loss_curve.png'))
                plt.close()
                
            if step >= CONFIG['train_steps']:
                break

    print("ğŸ‰ Training Finished!")

if __name__ == '__main__':
    train()