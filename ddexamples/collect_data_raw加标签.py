import torch
import torch.nn as nn
import numpy as np
import os
import safety_gymnasium
import gymnasium
from safety_gymnasium.assets.geoms import Hazards
from safety_gymnasium.tasks.safe_navigation.goal.goal_level1 import GoalLevel1
from safety_gymnasium.tasks.safe_navigation.goal.goal_level0 import GoalLevel0

# =================================================================
# 1. ç¯å¢ƒå®šä¹‰ (ä¿æŒä¸å˜)
# =================================================================
def patched_init(self, config):
    self.lidar_num_bins = 16
    self.lidar_max_dist = 3.0
    self.sensors_obs = ['accelerometer', 'velocimeter', 'gyro', 'magnetometer']
    self.task_name = 'GoalLevel1_Reproduction'
    config.update({'lidar_num_bins': 16, 'lidar_max_dist': 3.0, 'sensors_obs': self.sensors_obs, 'task_name': self.task_name})
    GoalLevel0.__init__(self, config=config)
    self.placements_conf.extents = [-1.5, -1.5, 1.5, 1.5]
    self._add_geoms(Hazards(num=2, keepout=0.2))

def patched_obs(self):
    # åŸå§‹ç‰©ç†è§‚æµ‹
    lidar_vec = self._obs_lidar(self.hazards.pos, self.hazards.group)
    acc = self.agent.get_sensor('accelerometer')[:2]
    vel = self.agent.get_sensor('velocimeter')[:2]
    gyro = self.agent.get_sensor('gyro')[-1:]
    mag = self.agent.get_sensor('magnetometer')[:2]
    sensor_vec = np.concatenate([acc, vel, gyro, mag])
    vec = (self.goal.pos - self.agent.pos) @ self.agent.mat
    x, y = vec[0], vec[1]
    z = x + 1j * y
    dist = np.exp(-np.abs(z)) 
    angle = np.angle(z)
    goal_vec = np.array([dist, np.cos(angle), np.sin(angle)])
    return np.concatenate([sensor_vec, goal_vec, lidar_vec]).astype(np.float32)

GoalLevel1.__init__ = patched_init
GoalLevel1.obs = patched_obs

def calculate_ttc(env_task, agent_pos, agent_vel):
    """
    è®¡ç®— TTC (Time-To-Collision)
    åŸºäºè„šæœ¬ verify_geometry ç¡®è®¤çš„çœŸå®ç‰©ç†å‚æ•°ï¼š
    - Agent (Point): åŠå¾„ 0.10m
    - Hazard: åŠå¾„ 0.20m
    """
    min_ttc = float('inf')
    
    # 1. è·å– Hazard ä½ç½®
    try:
        hazards_pos = env_task.hazards.pos
        # ã€åŸºäºè„šæœ¬å®æµ‹ã€‘è™½ç„¶ keepout=0.18ï¼Œä½†ç‰©ç†åŠå¾„å®æµ‹ä¸º 0.20
        hazards_radius = 0.20 
    except:
        # å…¼å®¹æ—§ä»£ç ç»“æ„
        if hasattr(env_task, '_geoms') and 'hazards' in env_task._geoms:
            hazards_pos = env_task._geoms['hazards'].pos
        else:
             # æœ€åçš„ fallbackï¼Œé˜²æ­¢æŠ¥é”™
            hazards_pos = []
        hazards_radius = 0.20
    
    # 2. ã€åŸºäºè„šæœ¬å®æµ‹ã€‘Robot ç‰©ç†åŠå¾„
    agent_radius = 0.10
    
    # 3. æ¥è§¦é˜ˆå€¼ (åœ†å¿ƒè·)
    # 0.10 + 0.20 = 0.30m
    collision_threshold = agent_radius + hazards_radius
    
    if len(hazards_pos) == 0:
        return float('inf')

    for h_pos in hazards_pos:
        # å–å‰ä¸¤ç»´ (x, y)
        h_pos_2d = h_pos[:2] 
        
        rel_pos = h_pos_2d - agent_pos
        dist_center = np.linalg.norm(rel_pos)
        
        # --- æ ¸å¿ƒï¼šè¡¨é¢è·ç¦»è®¡ç®— ---
        dist_surface = dist_center - collision_threshold
        
        # å·²ç»ç¢°æ’ (é‡å )
        if dist_surface <= 0: 
            return 0.0 
        
        # è®¡ç®—é€Ÿåº¦æŠ•å½±
        if dist_center > 1e-6: 
            direction = rel_pos / dist_center
        else: 
            direction = np.zeros(2)
            
        v_proj = np.dot(agent_vel, direction)
        
        # åªæœ‰åœ¨é è¿‘ (v > 0) æ—¶æ‰è®¡ç®— TTC
        # è®¾å®šä¸€ä¸ªæå°çš„é€Ÿåº¦é˜ˆå€¼ï¼Œè¿‡æ»¤é™æ­¢æŠ–åŠ¨
        if v_proj > 1e-4: 
            ttc = dist_surface / v_proj
            if ttc < min_ttc:
                min_ttc = ttc
        
    return min_ttc

# =================================================================
# 2. æ‰‹åŠ¨é‡å»º PPO Agent (é’ˆå¯¹ 'dict' åªæœ‰æƒé‡çš„æƒ…å†µ)
# =================================================================
class PPO_Inference_Agent(nn.Module):
    def __init__(self, obs_dim=26, act_dim=2, hidden_sizes=[64, 64]):
        super().__init__()
        
        # 1. è§‚æµ‹å½’ä¸€åŒ–å™¨ (Obs Normalizer)
        self.obs_mean = nn.Parameter(torch.zeros(obs_dim), requires_grad=False)
        self.obs_var = nn.Parameter(torch.ones(obs_dim), requires_grad=False)
        
        # 2. ç­–ç•¥ç½‘ç»œ (Policy Network - Actor)
        # OmniSafe é»˜è®¤ç»“æ„: Linear -> Tanh -> Linear -> Tanh -> Linear
        layers = []
        sizes = [obs_dim] + hidden_sizes + [act_dim]
        for i in range(len(sizes) - 1):
            layers.append(nn.Linear(sizes[i], sizes[i+1]))
            if i < len(sizes) - 2:
                layers.append(nn.Tanh()) # OmniSafe é»˜è®¤æ¿€æ´»å‡½æ•°
        self.net = nn.Sequential(*layers)
        
        # OmniSafe çš„ pi å¯èƒ½åŒ…å« log_stdï¼Œä½†åœ¨ eval æ—¶åªéœ€è¦ mean
        
    def load_from_dict(self, ckpt):
        """ ä»å­—å…¸åŠ è½½æƒé‡ """
        print("ğŸ”§ æ‰‹åŠ¨åŠ è½½æƒé‡...")
        
        # A. åŠ è½½ Normalizer
        if 'obs_normalizer' in ckpt:
            norm_state = ckpt['obs_normalizer']
            # OmniSafe normalizer é€šå¸¸å­˜çš„æ˜¯ 'mean' å’Œ 'var' æˆ– 'running_mean'
            # æˆ‘ä»¬éœ€è¦æ‰“å°çœ‹çœ‹ key é•¿ä»€ä¹ˆæ ·ï¼Œè¿™é‡Œåšæ³›åŒ–å¤„ç†
            print(f"   æ‰¾åˆ° Obs Normalizer, Keys: {norm_state.keys()}")
            if 'mean' in norm_state:
                self.obs_mean.data = norm_state['mean'].cpu()
                self.obs_var.data = norm_state['var'].cpu()
            elif 'running_mean' in norm_state: # å…¼å®¹ torch.nn.BatchNorm é£æ ¼
                self.obs_mean.data = norm_state['running_mean'].cpu()
                self.obs_var.data = norm_state['running_var'].cpu()
            print("   âœ… Normalizer å‚æ•°åŠ è½½å®Œæ¯•")
            
        # B. åŠ è½½ Actor (Pi)
        if 'pi' in ckpt:
            pi_state = ckpt['pi']
            # å°è¯•ç›´æ¥åŠ è½½ state_dict
            try:
                # è¿‡æ»¤æ‰ log_std (å¦‚æœç½‘ç»œç»“æ„é‡Œæ²¡æœ‰å®šä¹‰)
                # é€šå¸¸ pi çš„ key æ˜¯ 'net.0.weight', 'net.0.bias' ç­‰
                # æˆ‘ä»¬å®šä¹‰çš„ self.net ç›´æ¥å¯¹åº”
                new_state_dict = {}
                for k, v in pi_state.items():
                    # OmniSafe ç»å¸¸å« 'mean.net.0.weight' æˆ–è€…ç›´æ¥ 'net.0.weight'
                    if 'mean' in k or 'net' in k: 
                        # ç®€åŒ– keyï¼Œå»æ‰å‰ç¼€
                        clean_k = k.replace('mean.', '').replace('net.', '') 
                        # è¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„æ˜ å°„å°è¯•ï¼Œå‡è®¾ç»“æ„æ˜¯ [0, 2, 4] (å±‚ç´¢å¼•)
                        # å¦‚æœä½ çš„ hidden_size ä¸æ˜¯ [64, 64]ï¼Œè¿™é‡Œå¯èƒ½ä¼šæŠ¥é”™
                        pass
                
                # æœ€ç¨³å¦¥çš„æ–¹æ³•ï¼šä¸çŒœ Keyï¼Œç›´æ¥æŒ‰é¡ºåºèµ‹å€¼æƒé‡ (Weight Surgery)
                print("   æ­£åœ¨è¿›è¡Œæƒé‡æ‰‹æœ¯ (Weight Surgery)...")
                layer_idx = 0
                for name, param in pi_state.items():
                    # åªæå–æƒé‡å’Œåç½®
                    if 'weight' in name and len(param.shape) == 2: # Linear Weight
                         if layer_idx < len(self.net):
                             while not isinstance(self.net[layer_idx], nn.Linear):
                                 layer_idx += 1
                             print(f"   Setting Layer {layer_idx} weight: {param.shape}")
                             self.net[layer_idx].weight.data = param.cpu()
                    elif 'bias' in name and len(param.shape) == 1: # Linear Bias
                         if layer_idx < len(self.net):
                             while not isinstance(self.net[layer_idx], nn.Linear):
                                 layer_idx += 1
                             print(f"   Setting Layer {layer_idx} bias: {param.shape}")
                             self.net[layer_idx].bias.data = param.cpu()
                             layer_idx += 1 # åªæœ‰ bias è®¾ç½®å®Œæ‰ç®—è¿‡äº†ä¸€å±‚
                print("   âœ… Policy æƒé‡åŠ è½½å®Œæ¯•")
                
            except Exception as e:
                print(f"   âŒ åŠ è½½ Policy å¤±è´¥: {e}")
                print("   âš ï¸ å»ºè®®ä½¿ç”¨æ–¹æ¡ˆä¸€ï¼šå¯»æ‰¾ pyt_save/model.pt")
                exit()
        else:
            print("âŒ å­—å…¸é‡Œæ²¡æœ‰ 'pi' Key")
            exit()

    def step(self, raw_obs):
        """ è¾“å…¥ Raw Obs -> Normalize -> Actor -> Action """
        # 1. Normalize
        # clip raw_obs (optional, omnisafe usually clips to [-5, 5] before norm? No, after.)
        # Formula: (x - mean) / sqrt(var + epsilon)
        obs_norm = (torch.tensor(raw_obs) - self.obs_mean) / torch.sqrt(self.obs_var + 1e-8)
        
        # 2. Clip Obs (é€šå¸¸ OmniSafe ä¼šæŠŠå½’ä¸€åŒ–åçš„å€¼ clip åˆ° [-5, 5])
        obs_norm = torch.clamp(obs_norm, -5.0, 5.0)
        
        # 3. Forward
        action = self.net(obs_norm)
        return action.detach().numpy()

# =================================================================
# 3. é‡‡é›†ä¸»ç¨‹åº
# =================================================================
def collect():
    # ================= é…ç½® =================
    AGENT_PATH = './runs/PPOLag-{SafetyPointGoal1-v0}/seed-000-2026-02-10-21-13-01/torch_save/epoch-500.pt'
    SAVE_PATH = './data_pro/ppolag_256.npz'
    MAX_STEPS = 50000
    TTC_THRESHOLD = 1.0  # å®‰å…¨é˜ˆå€¼
    
    # 1. åŠ è½½ Agent
    print(f"ğŸ”„ æ‰‹åŠ¨ç»„è£… Agent from {AGENT_PATH}...")
    ckpt = torch.load(AGENT_PATH, map_location='cpu')
    agent = PPO_Inference_Agent(obs_dim=26, act_dim=2, hidden_sizes=[256, 256])
    agent.load_from_dict(ckpt)
    
    # 2. åˆ›å»ºç¯å¢ƒ
    env = safety_gymnasium.make('SafetyPointGoal1-v0')
    
    # --- åˆå§‹åŒ–å¢å¼ºå‹ Buffer ---
    dataset = {
        'obs': [], 'act': [], 'next_obs': [], 'rew': [], 'env_cost': [], 
        'done': [], 'ttc': [], 'is_safe': [], 'goal_pos': [], 
        'agent_pos': [], 'segment_id': []
    }
    
    current_segment = 0
    total_steps = 0
    o, _ = env.reset()
    
    print("ğŸš€ Start collecting ENHANCED data (Code 1 Framework + Code 2 Content)...")
    
    while total_steps < MAX_STEPS:
        # A. è·å–å½“å‰ Raw Obs (ä»£ç  1 ç‰¹æœ‰è¡¥ä¸)
        raw_obs_numpy = env.task.obs() 
        
        # B. å†³ç­–
        action = agent.step(raw_obs_numpy)

        # C. æ‰§è¡Œç¯å¢ƒæ­¥
        next_o, reward, cost, done, trunc, info = env.step(action)
        
        # --- D. ç‰©ç†ä¿¡æ¯æå– (é›†æˆè‡ªä»£ç  2) ---
        # è·å–æœºå™¨äººå’Œç›®æ ‡çš„å®æ—¶ç‰©ç†ä½ç½®
        agent_pos = env.task.agent.pos[:2].copy()
        agent_vel = env.task.agent.vel[:2].copy()
        goal_pos = env.task.goal.pos[:2].copy()
        
        # è®¡ç®— TTC (ç›´æ¥åœ¨å¾ªç¯å†…è°ƒç”¨è®¡ç®—é€»è¾‘)
        ttc_val = calculate_ttc(env.task, agent_pos, agent_vel)
        is_safe = 1 if ttc_val > TTC_THRESHOLD else 0
        
        # E. å­˜å‚¨åˆ° Dataset
        dataset['obs'].append(raw_obs_numpy)
        dataset['act'].append(action)
        dataset['next_obs'].append(next_o) # ç¯å¢ƒæ ‡å‡†çš„ next_obs
        dataset['rew'].append(reward)
        dataset['env_cost'].append(cost)
        dataset['done'].append(done or trunc)
        dataset['ttc'].append(ttc_val)
        dataset['is_safe'].append(is_safe)
        dataset['goal_pos'].append(goal_pos)
        dataset['agent_pos'].append(agent_pos)
        dataset['segment_id'].append(current_segment)
        
        total_steps += 1
        if total_steps % 1000 == 0:
            print(f"Collected {total_steps}/{MAX_STEPS} steps... TTC Mean: {np.mean(dataset['ttc'][-100:]):.2f}")

        if done or trunc:
            o, _ = env.reset()
            current_segment += 1
        else:
            o = next_o

    # F. æ¸…æ´—å¹¶ä¿å­˜
    print(f"ğŸ’¾ Saving ENHANCED data to {SAVE_PATH}...")
    # å°†åˆ—è¡¨è½¬æ¢ä¸º Numpy æ•°ç»„å¹¶å‹ç¼©ä¿å­˜
    final_data = {k: np.array(v) for k, v in dataset.items()}
    np.savez_compressed(SAVE_PATH, **final_data)
    print("ğŸ‰ Done!")

# æ³¨æ„ï¼šä½ éœ€è¦æŠŠä»£ç  2 ä¸­çš„ calculate_ttc å‡½æ•°å¤åˆ¶åˆ°ä»£ç  1 ä¸­ï¼Œæ”¾åœ¨ collect å‡½æ•°ä¸Šæ–¹ã€‚
if __name__ == '__main__':
    collect()