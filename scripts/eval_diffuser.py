# # import os
# # import torch
# # import numpy as np
# # import omnisafe
# # import imageio
# # import sys
# # import time
# # import pickle
# # import gymnasium
# # from collections import namedtuple
# # # /home/lqz27/anaconda3/envs/omnisafedd/bin/python /home/lqz27/dyx_ws/omnisafe/scripts/eval_diffuser.py
# # # ================= è·¯å¾„è®¾ç½® =================
# # current_dir = os.path.dirname(os.path.abspath(__file__))
# # project_root = os.path.dirname(current_dir)
# # if project_root not in sys.path: sys.path.append(project_root)
# # sys.path.append(current_dir) 

# # from diffuser.models.diffusion import GaussianDiffusion
# # from diffuser.models.temporal import TemporalUnet
# # from dataset_adapter import SafetyGymDataset

# # # ================= Monkey Patch (ä¿æŒç¯å¢ƒä¸€è‡´) =================
# # import safety_gymnasium
# # from safety_gymnasium.assets.geoms import Hazards
# # from safety_gymnasium.tasks.safe_navigation.goal.goal_level1 import GoalLevel1
# # from safety_gymnasium.tasks.safe_navigation.goal.goal_level0 import GoalLevel0

# # def patched_init(self, config):
# #     self.lidar_num_bins = 16
# #     self.lidar_max_dist = 3.0
# #     self.sensors_obs = ['accelerometer', 'velocimeter', 'gyro', 'magnetometer']
# #     self.task_name = 'GoalLevel1_Reproduction'
# #     config.update({'lidar_num_bins': 16, 'lidar_max_dist': 3.0, 
# #                    'sensors_obs': self.sensors_obs, 'task_name': self.task_name})
# #     GoalLevel0.__init__(self, config=config)
# #     self.placements_conf.extents = [-1.5, -1.5, 1.5, 1.5]
# #     self._add_geoms(Hazards(num=2, keepout=0.18))

# # def patched_build_observation_space(self):
# #     self.observation_space = gymnasium.spaces.Box(low=-np.inf, high=np.inf, shape=(26,), dtype=np.float32)

# # def patched_obs(self):
# #     lidar_vec = self._obs_lidar(self.hazards.pos, self.hazards.group) 
# #     acc = self.agent.get_sensor('accelerometer')[:2]
# #     vel = self.agent.get_sensor('velocimeter')[:2]
# #     gyro = self.agent.get_sensor('gyro')[-1:]
# #     mag = self.agent.get_sensor('magnetometer')[:2]
# #     sensor_vec = np.concatenate([acc, vel, gyro, mag])
# #     vec = (self.goal.pos - self.agent.pos) @ self.agent.mat
# #     x, y = vec[0], vec[1]
# #     z = x + 1j * y
# #     dist = np.exp(-np.abs(z)) 
# #     angle = np.angle(z)
# #     goal_vec = np.array([dist, np.cos(angle), np.sin(angle)])
# #     return np.concatenate([sensor_vec, goal_vec, lidar_vec]).astype(np.float32)

# # GoalLevel1.__init__ = patched_init
# # GoalLevel1.build_observation_space = patched_build_observation_space
# # GoalLevel1.obs = patched_obs

# # # ================= é…ç½®åŒºåŸŸ =================
# # DATASET_NAME = 'dataset_raw.npz' # ä½¿ç”¨ Raw è®­ç»ƒçš„æ¨¡å‹
# # # DATASET_NAME = 'dataset_safe_only.npz' 

# # CHECKPOINT_DIR = os.path.join(project_root, 'diffuser_checkpoints', DATASET_NAME.replace(".npz", ""))
# # CHECKPOINT_NAME = None
# # DATASET_PATH = os.path.join(project_root, 'datasets', DATASET_NAME)

# # DEVICE = 'cuda:0'
# # HORIZON = 64
# # NUM_EPISODES = 5 # è·‘å‡ è½®æµ‹è¯•

# # # ================= ç­–ç•¥å°è£… (ä»¿ç…§ä½ çš„ Policy ç±») =================
# # class DiffusionPolicy:
# #     def __init__(self, diffusion_model, normalizer):
# #         self.diffusion = diffusion_model
# #         self.normalizer = normalizer
# #         self.action_dim = diffusion_model.action_dim
# #         self.observation_dim = diffusion_model.observation_dim

# #     def __call__(self, obs, batch_size=1):
# #         # 1. å½’ä¸€åŒ–è§‚æµ‹
# #         # obs: (26,) -> (1, 26)
# #         obs_in = obs[None, :]
# #         norm_obs = self.normalizer.normalize(obs_in, 'observations')
        
# #         # 2. æ„é€ æ¡ä»¶
# #         conditions = {0: torch.tensor(norm_obs, device=DEVICE)}
        
# #         # 3. ç”Ÿæˆè½¨è¿¹
# #         start_t = time.time()
# #         # with torch.no_grad():
# #         #     # samples: (B, H, Obs+Act)
# #         #     samples = self.diffusion.conditional_sample(conditions)
# #         #     samples = samples.cpu().numpy()
# #         with torch.no_grad():
# #             samples = self.diffusion.conditional_sample(conditions, return_diffusion=False, verbose=False)
# #             if isinstance(samples, tuple):
# #                 samples = samples[0]
                
# #             samples = samples.cpu().numpy()
# #         end_t = time.time()
        
# #         # 4. æå–åŠ¨ä½œ (Trajectory Optimization)
# #         # æå–ç¬¬ä¸€æ­¥çš„åŠ¨ä½œéƒ¨åˆ† [0, 0, 26:28]
# #         norm_action = samples[0, 0, self.observation_dim:] 
        
# #         # 5. åå½’ä¸€åŒ–åŠ¨ä½œ
# #         action = self.normalizer.unnormalize(norm_action[None, :], 'actions')
# #         action = action[0]

# #         if np.random.rand() < 0.05: # 5% çš„æ¦‚ç‡æ‰“å°ï¼ŒæŠ½æŸ¥
# #             print(f"\nğŸ” [è¯Šæ–­] Step æ£€æŸ¥:")
# #             print(f"  > æ¨¡å‹è¾“å‡º (Norm): {norm_action} (èŒƒå›´åº”åœ¨ -1~1 ä¹‹é—´)")
# #             print(f"  > è¿˜åŸåŠ¨ä½œ (Real): {action} (Pointæœºå™¨äººé€šå¸¸åœ¨ -1~1 ä¹‹é—´)")
# #             print(f"  > æ•°æ®é›†åŠ¨ä½œèŒƒå›´: Min={self.normalizer.act_min}, Max={self.normalizer.act_max}")
        
# #         return action, samples, (end_t - start_t)

# # # ================= ä¸»é€»è¾‘ =================
# # def main():
# #     # 1. åŠ è½½æ•°æ®é›† (Stats)
# #     print(f"Loading dataset stats: {DATASET_PATH}")
# #     dataset = SafetyGymDataset(DATASET_PATH, horizon=HORIZON)
    
# #     # 2. åŠ è½½æ¨¡å‹
# #     model = TemporalUnet(
# #         horizon=HORIZON,
# #         transition_dim=26 + 2,
# #         cond_dim=26,
# #         dim=256,
# #         dim_mults=(1, 2, 4)
# #     ).to(DEVICE)

# #     diffusion = GaussianDiffusion(
# #         model=model,
# #         horizon=HORIZON,
# #         observation_dim=26,
# #         action_dim=2,
# #         n_timesteps=20, 
# #         loss_type='l2',
# #         clip_denoised=True,
# #         predict_epsilon=True,
# #     ).to(DEVICE)
# #     diffusion.normalizer = dataset

# #     # 3. åŠ è½½æƒé‡
# #     if CHECKPOINT_NAME is None:
# #         # è·å–ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
# #         all_files = os.listdir(CHECKPOINT_DIR)
# #         # ç­›é€‰å‡º "state_xxx.pt" æ ¼å¼çš„æ–‡ä»¶
# #         ckpt_files = [f for f in all_files if f.startswith('state_') and f.endswith('.pt')]
        
# #         if not ckpt_files:
# #             raise FileNotFoundError(f"âŒ åœ¨ {CHECKPOINT_DIR} ä¸‹æ²¡æ‰¾åˆ°ä»»ä½• state_*.pt æ¨¡å‹æ–‡ä»¶ï¼è¯·å…ˆè®­ç»ƒã€‚")
        
# #         # æå–æ­¥æ•°å¹¶æ’åº (state_1000.pt -> 1000)
# #         # key é€»è¾‘: æŠŠ "state_" å’Œ ".pt" å»æ‰ï¼Œå‰©ä¸‹çš„è½¬ int
# #         ckpt_files.sort(key=lambda x: int(x.replace('state_', '').replace('.pt', '')))
        
# #         # å–æœ€åä¸€ä¸ª (æ­¥æ•°æœ€å¤§çš„)
# #         latest_ckpt = ckpt_files[-1]
# #         print(f"ğŸ”„ è‡ªåŠ¨æ£€æµ‹åˆ°æœ€æ–°æ¨¡å‹: {latest_ckpt}")
# #         ckpt_path = os.path.join(CHECKPOINT_DIR, latest_ckpt)
# #     else:
# #         # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶åï¼Œå°±ç”¨æŒ‡å®šçš„
# #         ckpt_path = os.path.join(CHECKPOINT_DIR, CHECKPOINT_NAME)

# #     print(f"Loading checkpoint: {ckpt_path}")
# #     state_dict = torch.load(ckpt_path, map_location=DEVICE)
# #     # diffusion.model.load_state_dict(state_dict['model']) # æˆ–è€… ['ema']
# #     diffusion.load_state_dict(state_dict['model'])
# #     diffusion.eval()
    
# #     # åˆå§‹åŒ–ç­–ç•¥
# #     policy = DiffusionPolicy(diffusion, dataset)

# #     # 4. ğŸ”¥ é¢„çƒ­ GPU (æ¥è‡ª planlast.py çš„çµæ„Ÿ)
# #     print("ğŸ”¥ Warming up GPU...")
# #     dummy_obs = np.zeros(26, dtype=np.float32)
# #     for _ in range(2):
# #         policy(dummy_obs)
# #     print("âœ… Warmup done.")

# #     # 5. ç¯å¢ƒ
# #     # evaluator = omnisafe.Evaluator(render_mode='rgb_array')
# #     # env = evaluator._env 
# #     env = safety_gymnasium.make('SafetyPointGoal1-v0', render_mode='rgb_array', camera_name='fixedfar', width=256, height=256)
    
# #     # 6. è¯„ä¼°å¾ªç¯
# #     results = []
    
# #     for ep in range(NUM_EPISODES):
# #         print(f"\n=== Episode {ep+1}/{NUM_EPISODES} ===")
# #         obs, _ = env.reset()
# #         done = False
# #         total_rew = 0
# #         total_cost = 0
# #         step = 0
        
# #         # è§†é¢‘å½•åˆ¶
# #         video_path = f'eval_ep{ep}.mp4'
# #         writer = imageio.get_writer(video_path, fps=30)
        
# #         traj_data = {'observations': [], 'actions': [], 'costs': []}
        
# #         while not done and step < 1000:
# #             # Plan
# #             action, plan_traj, plan_time = policy(obs)
            
# #             # Step
# #             act_tensor = torch.as_tensor(action, dtype=torch.float32)
# #             next_obs, reward, cost, terminated, truncated, _ = env.step(act_tensor)
            
# #             # Record
# #             frame = env.render()
# #             writer.append_data(frame)
            
# #             traj_data['observations'].append(obs)
# #             traj_data['actions'].append(action)
# #             traj_data['costs'].append(cost)
            
# #             total_rew += reward
# #             total_cost += cost
# #             obs = next_obs
# #             step += 1
            
# #             if step % 50 == 0:
# #                 print(f"Step {step} | Reward: {reward:.3f} | Cost: {cost:.0f}")
                
# #             if terminated or truncated:
# #                 done = True
                
# #         writer.close()
# #         print(f"Episode Finished. Return: {total_rew:.2f}, Cost: {total_cost}")
        
# #         # ä¿å­˜è¿™ä¸€è½®çš„æ•°æ® (å‚è€ƒ planlast çš„ all_raw_results)
# #         results.append({
# #             'episode': ep,
# #             'return': total_rew,
# #             'cost': total_cost,
# #             'length': step,
# #             'trajectory': np.array(traj_data['observations'])
# #         })

# #     # 7. ä¿å­˜æœ€ç»ˆç»“æœåˆ° PKL
# #     save_pkl = os.path.join(project_root, f'eval_results_{DATASET_NAME[:-4]}.pkl')
# #     with open(save_pkl, 'wb') as f:
# #         pickle.dump(results, f)
# #     print(f"\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜è‡³: {save_pkl}")

# # if __name__ == '__main__':
# #     main()

# import os
# # ã€æ ¸å¿ƒä¿®å¤ 1ã€‘å¼ºåˆ¶ä½¿ç”¨ EGL åç«¯è¿›è¡Œæ— å¤´æ¸²æŸ“ (å¿…é¡»æ”¾åœ¨ import imageio ä¹‹å‰)
# os.environ['MUJOCO_GL'] = 'egl' 

# import torch
# import numpy as np
# import imageio
# import sys
# import time
# import pickle
# import gymnasium
# from collections import namedtuple

# # ================= è·¯å¾„è®¾ç½® =================
# current_dir = os.path.dirname(os.path.abspath(__file__))
# project_root = os.path.dirname(current_dir)
# if project_root not in sys.path: sys.path.append(project_root)
# sys.path.append(current_dir) 

# from diffuser.models.diffusion import GaussianDiffusion
# from diffuser.models.temporal import TemporalUnet
# from scripts.dataset_adapter import SafetyGymDataset # ç¡®ä¿è·¯å¾„æ­£ç¡®

# # ================= Monkey Patch (ä¿æŒç¯å¢ƒä¸€è‡´) =================
# import safety_gymnasium
# from safety_gymnasium.assets.geoms import Hazards
# from safety_gymnasium.tasks.safe_navigation.goal.goal_level1 import GoalLevel1
# from safety_gymnasium.tasks.safe_navigation.goal.goal_level0 import GoalLevel0

# def patched_init(self, config):
#     self.lidar_num_bins = 16
#     self.lidar_max_dist = 3.0
#     self.sensors_obs = ['accelerometer', 'velocimeter', 'gyro', 'magnetometer']
#     self.task_name = 'GoalLevel1_Reproduction'
#     config.update({'lidar_num_bins': 16, 'lidar_max_dist': 3.0, 
#                    'sensors_obs': self.sensors_obs, 'task_name': self.task_name})
#     GoalLevel0.__init__(self, config=config)
#     self.placements_conf.extents = [-1.5, -1.5, 1.5, 1.5]
#     self._add_geoms(Hazards(num=2, keepout=0.18))

# def patched_build_observation_space(self):
#     self.observation_space = gymnasium.spaces.Box(low=-np.inf, high=np.inf, shape=(26,), dtype=np.float32)

# def patched_obs(self):
#     lidar_vec = self._obs_lidar(self.hazards.pos, self.hazards.group) 
#     acc = self.agent.get_sensor('accelerometer')[:2]
#     vel = self.agent.get_sensor('velocimeter')[:2]
#     gyro = self.agent.get_sensor('gyro')[-1:]
#     mag = self.agent.get_sensor('magnetometer')[:2]
#     sensor_vec = np.concatenate([acc, vel, gyro, mag])
#     vec = (self.goal.pos - self.agent.pos) @ self.agent.mat
#     x, y = vec[0], vec[1]
#     z = x + 1j * y
#     dist = np.exp(-np.abs(z)) 
#     angle = np.angle(z)
#     goal_vec = np.array([dist, np.cos(angle), np.sin(angle)])
#     return np.concatenate([sensor_vec, goal_vec, lidar_vec]).astype(np.float32)

# GoalLevel1.__init__ = patched_init
# GoalLevel1.build_observation_space = patched_build_observation_space
# GoalLevel1.obs = patched_obs

# # ================= é…ç½®åŒºåŸŸ =================
# DATASET_NAME = 'dataset_raw.npz' # ä½¿ç”¨ Raw è®­ç»ƒçš„æ¨¡å‹
# # DATASET_NAME = 'dataset_safe_only.npz' 

# CHECKPOINT_DIR = os.path.join(project_root, 'diffuser_checkpoints', DATASET_NAME.replace(".npz", ""))
# # è®¾ç½®ä¸º None è¡¨ç¤ºè‡ªåŠ¨å¯»æ‰¾æ­¥æ•°æœ€å¤§çš„æ¨¡å‹
# CHECKPOINT_NAME = None 
# DATASET_PATH = os.path.join(project_root, 'datasets', DATASET_NAME)

# DEVICE = 'cuda:0'
# HORIZON = 64
# NUM_EPISODES = 5 # è·‘å‡ è½®æµ‹è¯•

# # ================= ç­–ç•¥å°è£… =================
# class DiffusionPolicy:
#     def __init__(self, diffusion_model, normalizer):
#         self.diffusion = diffusion_model
#         self.normalizer = normalizer
#         self.action_dim = diffusion_model.action_dim
#         self.observation_dim = diffusion_model.observation_dim

#     def __call__(self, obs, batch_size=1):
#         # 1. å½’ä¸€åŒ–è§‚æµ‹ (ä½¿ç”¨ Gaussian Mean/Std)
#         # obs: (26,) -> (1, 26)
#         obs_in = obs[None, :]
#         norm_obs = self.normalizer.normalize(obs_in, 'observations')
        
#         norm_obs = np.clip(norm_obs, -5.0, 5.0)
        
#         # 2. æ„é€ æ¡ä»¶
#         # æ³¨æ„ï¼šç°åœ¨ dataset_adapter é‡Œçš„æ¡ä»¶æ˜¯ {0: ...}
#         conditions = {0: torch.tensor(norm_obs, device=DEVICE)}
        
#         # 3. ç”Ÿæˆè½¨è¿¹
#         start_t = time.time()
#         with torch.no_grad():
#             # è¿™é‡Œçš„ verbose=False å¾ˆé‡è¦ï¼Œé˜²æ­¢è¿›åº¦æ¡åˆ·å±å¡æ­»
#             samples = self.diffusion.conditional_sample(conditions, return_diffusion=False, verbose=False)
            
#             if isinstance(samples, tuple):
#                 samples = samples[0]
                
#             samples = samples.cpu().numpy()
#         end_t = time.time()
        
#         # 4. æå–åŠ¨ä½œ (Trajectory Optimization)
#         # æå–ç¬¬ä¸€æ­¥çš„åŠ¨ä½œéƒ¨åˆ† [0, 0, 26:28] (å‰26æ˜¯Obs, å2æ˜¯Act)
#         # æ³¨æ„ï¼šç°åœ¨ samples çš„ç»´åº¦æ˜¯ (Batch, Horizon, Obs+Act) = (1, 64, 28)
#         norm_action = samples[0, 0, self.observation_dim:] 
        
#         # 5. åå½’ä¸€åŒ–åŠ¨ä½œ
#         action = self.normalizer.unnormalize(norm_action[None, :], 'actions')
#         action = action[0]

#         # # è¯Šæ–­æ‰“å° (Gaussian ç‰ˆ)
#         # if np.random.rand() < 0.1: # 5% çš„æ¦‚ç‡æ‰“å°ï¼ŒæŠ½æŸ¥
#         #     print(f"\nğŸ” [è¯Šæ–­] Step æ£€æŸ¥:")
#         #     # æ–°å¢ï¼šæ‰“å°è§‚æµ‹å€¼çš„å‰3ä½ï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯ä¹Ÿåœ¨å˜
#         #     print(f"  > è¾“å…¥è§‚æµ‹ (Norm Top3): {norm_obs[0, :3]}") 
#         #     print(f"  > æ¨¡å‹è¾“å‡º (Norm): {norm_action} (Gaussian: é€šå¸¸åœ¨ -3~3 ä¹‹é—´)")
#         #     print(f"  > è¿˜åŸåŠ¨ä½œ (Real): {action} (Pointæœºå™¨äººé€šå¸¸åœ¨ -1~1 ä¹‹é—´)")
#         #     # è¿™é‡Œçš„ act_min/max å…¶å®æ˜¯æˆªæ–­åçš„è¾¹ç•Œï¼Œä»…ä½œå‚è€ƒ
#         #     # print(f"  > æ•°æ®é›†åŠ¨ä½œè¾¹ç•Œ: Min={self.normalizer.act_min}, Max={self.normalizer.act_max}")
#         if np.random.rand() < 0.1: 
#             print(f"\nğŸ” [è¯Šæ–­] Step æ£€æŸ¥:")
#             # æ‰“å°è§‚æµ‹å€¼çš„ç¬¬ 20~23 ä½ (é€šå¸¸ Goal ä¿¡æ¯åœ¨è¿™é‡Œ)
#             # æ ¹æ® patched_obs: [sensor(16), goal(3), lidar(16)] -> wait, let's check index
#             # sensor_vec = 4 (acc+vel+gyro+mag) ?? No.
#             # è®©æˆ‘ä»¬ç›´æ¥æ‰“å° norm_obs çš„æ–¹å·®ï¼Œå¦‚æœå…¨æ˜¯ 0 å°±å®Œäº†
            
#             print(f"  > è¾“å…¥è§‚æµ‹ (Norm Min/Max): {norm_obs.min():.3f} / {norm_obs.max():.3f}")
#             # PointGoal1 çš„ obs ç»“æ„é€šå¸¸æ˜¯: 
#             # 0-3: ä¼ æ„Ÿå™¨
#             # 4-6: Goal (Dist, Cos, Sin) <--- é‡ç‚¹çœ‹è¿™é‡Œï¼
#             # 7-22: Lidar
#             print(f"  > ç›®æ ‡ä¿¡å· (Obs[7:10]): {norm_obs[0, 7:10]}") 
            
#             print(f"  > æ¨¡å‹è¾“å‡º (Norm): {norm_action}")
#             print(f"  > è¿˜åŸåŠ¨ä½œ (Real): {action}")
#         return action, samples, (end_t - start_t)

# # ================= ä¸»é€»è¾‘ =================
# def main():
#     # 1. åŠ è½½æ•°æ®é›† (Stats)
#     print(f"Loading dataset stats: {DATASET_PATH}")
#     # è¿™é‡Œä¼šè‡ªåŠ¨è¿›è¡Œ Gaussian ç»Ÿè®¡é‡çš„è®¡ç®—å’Œ Clip æ“ä½œ
#     dataset = SafetyGymDataset(DATASET_PATH, horizon=HORIZON)
    
#     # 2. åŠ è½½æ¨¡å‹
#     model = TemporalUnet(
#         horizon=HORIZON,
#         transition_dim=26 + 2, # Obs + Act
#         cond_dim=26,           # Obs
#         dim=256,
#         dim_mults=(1, 2, 4)
#     ).to(DEVICE)

#     diffusion = GaussianDiffusion(
#         model=model,
#         horizon=HORIZON,
#         observation_dim=26,
#         action_dim=2,
#         n_timesteps=100, 
#         loss_type='l2',
#         clip_denoised=True,
#         predict_epsilon=False,
#     ).to(DEVICE)
#     # ç»‘å®š normalizer
#     diffusion.normalizer = dataset

#     # 3. åŠ è½½æƒé‡ (è‡ªåŠ¨å¯»æ‰¾æœ€æ–°)
#     if CHECKPOINT_NAME is None:
#         if not os.path.exists(CHECKPOINT_DIR):
#              raise FileNotFoundError(f"âŒ ç›®å½•ä¸å­˜åœ¨: {CHECKPOINT_DIR}")
        
#         # è·å–ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
#         all_files = os.listdir(CHECKPOINT_DIR)
#         # ç­›é€‰å‡º "state_xxx.pt" æ ¼å¼çš„æ–‡ä»¶
#         ckpt_files = [f for f in all_files if f.startswith('state_') and f.endswith('.pt')]
        
#         if not ckpt_files:
#             raise FileNotFoundError(f"âŒ åœ¨ {CHECKPOINT_DIR} ä¸‹æ²¡æ‰¾åˆ°ä»»ä½• state_*.pt æ¨¡å‹æ–‡ä»¶ï¼è¯·å…ˆè®­ç»ƒã€‚")
        
#         # æå–æ­¥æ•°å¹¶æ’åº (state_1000.pt -> 1000)
#         ckpt_files.sort(key=lambda x: int(x.replace('state_', '').replace('.pt', '')))
        
#         # å–æœ€åä¸€ä¸ª (æ­¥æ•°æœ€å¤§çš„)
#         latest_ckpt = ckpt_files[-1]
#         print(f"ğŸ”„ è‡ªåŠ¨æ£€æµ‹åˆ°æœ€æ–°æ¨¡å‹: {latest_ckpt}")
#         ckpt_path = os.path.join(CHECKPOINT_DIR, latest_ckpt)
#     else:
#         # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶åï¼Œå°±ç”¨æŒ‡å®šçš„
#         ckpt_path = os.path.join(CHECKPOINT_DIR, CHECKPOINT_NAME)

#     print(f"Loading checkpoint: {ckpt_path}")
#     state_dict = torch.load(ckpt_path, map_location=DEVICE)
    
#     # ä¼˜å…ˆåŠ è½½ EMA æƒé‡ (å¦‚æœæœ‰)
#     # if 'ema' in state_dict:
#     #     print("âœ¨ Loading EMA weights (Better performance)...")
#     #     diffusion.load_state_dict(state_dict['ema'])
#     # else:
#     #     print("âš ï¸ No EMA weights found, loading standard weights...")
#     #     diffusion.load_state_dict(state_dict['model'])
#     print("âš ï¸ Force loading standard weights for debugging...")
#     diffusion.load_state_dict(state_dict['model'])
        
#     diffusion.eval()
    
#     # åˆå§‹åŒ–ç­–ç•¥
#     policy = DiffusionPolicy(diffusion, dataset)

#     # 4. ğŸ”¥ é¢„çƒ­ GPU
#     print("ğŸ”¥ Warming up GPU...")
#     dummy_obs = np.zeros(26, dtype=np.float32)
#     for _ in range(2):
#         policy(dummy_obs)
#     print("âœ… Warmup done.")

#     # 5. ç¯å¢ƒ (ç›´æ¥ makeï¼Œåº”ç”¨ Patch)
#     print("Creating environment...")
#     env = safety_gymnasium.make('SafetyPointGoal1-v0', render_mode='rgb_array', camera_name='fixedfar', width=256, height=256)
    
#     # 6. è¯„ä¼°å¾ªç¯
#     results = []
    
#     for ep in range(NUM_EPISODES):
#         print(f"\n=== Episode {ep+1}/{NUM_EPISODES} ===")
#         obs, _ = env.reset()
#         done = False
#         total_rew = 0
#         total_cost = 0
#         step = 0
        
#         # è§†é¢‘å½•åˆ¶
#         video_path = f'eval_ep{ep}.mp4'
#         # ä½¿ç”¨ imageio çš„ ffmpeg writerï¼ŒæŒ‡å®š pixel format ä»¥å…¼å®¹å¤§å¤šæ•°æ’­æ”¾å™¨
#         writer = imageio.get_writer(video_path, fps=30, macro_block_size=None)
        
#         traj_data = {'observations': [], 'actions': [], 'costs': []}
        
#         while not done and step < 1000:
#             # Plan
#             action, plan_traj, plan_time = policy(obs)
            
#             # Step
#             # å†æ¬¡å¼ºåˆ¶ Clip åŠ¨ä½œï¼Œé˜²æ­¢ç‰©ç†å¼•æ“ç‚¸è£‚
#             action = np.clip(action, -1.0, 1.0)
            
#             act_tensor = torch.as_tensor(action, dtype=torch.float32)
#             # Safety Gym æ¥å£å˜æ›´: reset è¿”å› info, step è¿”å› terminated, truncated
#             # next_obs, reward, cost, terminated, truncated, info
#             step_result = env.step(act_tensor)
            
#             if len(step_result) == 6: # New Gym API
#                  next_obs, reward, cost, terminated, truncated, _ = step_result
#             elif len(step_result) == 5: # Old Gym API (safety gym å¯èƒ½ä¼šæœ‰å˜ç§)
#                  next_obs, reward, cost, done, _ = step_result
#                  terminated = done
#                  truncated = False
#             elif len(step_result) == 4: # Standard Gym
#                  next_obs, reward, terminated, truncated = step_result
#                  cost = 0 # æ²¡æœ‰ Cost

#             # Record
#             try:
#                 frame = env.render()
#                 writer.append_data(frame)
#             except Exception as e:
#                 if step == 0: print(f"âš ï¸ Render failed: {e}")

#             traj_data['observations'].append(obs)
#             traj_data['actions'].append(action)
#             traj_data['costs'].append(cost)
            
#             total_rew += reward
#             total_cost += cost
#             obs = next_obs
#             step += 1
            
#             if step % 50 == 0:
#                 print(f"Step {step} | Reward: {reward:.3f} | Cost: {cost:.0f}")
                
#             if terminated or truncated:
#                 done = True
                
#         writer.close()
#         print(f"Episode Finished. Return: {total_rew:.2f}, Cost: {total_cost}")
        
#         # ä¿å­˜è¿™ä¸€è½®çš„æ•°æ®
#         results.append({
#             'episode': ep,
#             'return': total_rew,
#             'cost': total_cost,
#             'length': step,
#             'trajectory': np.array(traj_data['observations'])
#         })

#     # 7. ä¿å­˜æœ€ç»ˆç»“æœåˆ° PKL
#     save_pkl = os.path.join(project_root, f'eval_results_{DATASET_NAME[:-4]}.pkl')
#     with open(save_pkl, 'wb') as f:
#         pickle.dump(results, f)
#     print(f"\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜è‡³: {save_pkl}")

# if __name__ == '__main__':
#     main()



import os
# ã€æ ¸å¿ƒä¿®å¤ 1ã€‘å¼ºåˆ¶ä½¿ç”¨ EGL åç«¯è¿›è¡Œæ— å¤´æ¸²æŸ“ (å¿…é¡»æ”¾åœ¨ import imageio ä¹‹å‰)
os.environ['MUJOCO_GL'] = 'egl' 

import torch
import numpy as np
import imageio
import sys
import time
import pickle
import gymnasium
import argparse  # ğŸ”¥ã€æ–°å¢ã€‘å‚æ•°è§£æ
from collections import namedtuple

# ================= è·¯å¾„è®¾ç½® =================
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path: sys.path.append(project_root)
sys.path.append(current_dir) 

from diffuser.models.diffusion import GaussianDiffusion
from diffuser.models.temporal import TemporalUnet
from scripts.dataset_adapter import SafetyGymDataset 

# ================= Monkey Patch (ä¿æŒç¯å¢ƒä¸€è‡´) =================
import safety_gymnasium
from safety_gymnasium.assets.geoms import Hazards
from safety_gymnasium.tasks.safe_navigation.goal.goal_level1 import GoalLevel1
from safety_gymnasium.tasks.safe_navigation.goal.goal_level0 import GoalLevel0

def patched_init(self, config):
    self.lidar_num_bins = 16
    self.lidar_max_dist = 3.0
    self.sensors_obs = ['accelerometer', 'velocimeter', 'gyro', 'magnetometer']
    self.task_name = 'GoalLevel1_Reproduction'
    config.update({'lidar_num_bins': 16, 'lidar_max_dist': 3.0, 
                   'sensors_obs': self.sensors_obs, 'task_name': self.task_name})
    GoalLevel0.__init__(self, config=config)
    self.placements_conf.extents = [-1.5, -1.5, 1.5, 1.5]
    self._add_geoms(Hazards(num=2, keepout=0.2))

def patched_build_observation_space(self):
    self.observation_space = gymnasium.spaces.Box(low=-np.inf, high=np.inf, shape=(26,), dtype=np.float32)

def patched_obs(self):
    lidar_vec = self._obs_lidar(self.hazards.pos, self.hazards.group) 
    acc = self.agent.get_sensor('accelerometer')[:2]
    vel = self.agent.get_sensor('velocimeter')[:2]
    gyro = self.agent.get_sensor('gyro')[-1:]
    mag = self.agent.get_sensor('magnetometer')[:2]
    sensor_vec = np.concatenate([acc, vel, gyro, mag])
    vec = (self.goal.pos - self.agent.pos) @ self.agent.mat
    x, y = vec[0], vec[1]
    z = x + 1j * y
    dist = np.exp(-np.abs(z)) 
    angle = np.angle(z)
    goal_vec = np.array([dist, np.cos(angle), np.sin(angle)])
    return np.concatenate([sensor_vec, goal_vec, lidar_vec]).astype(np.float32)

GoalLevel1.__init__ = patched_init
GoalLevel1.build_observation_space = patched_build_observation_space
GoalLevel1.obs = patched_obs

# ================= ç­–ç•¥å°è£… =================
class DiffusionPolicy:
    def __init__(self, diffusion_model, normalizer, goal_weight=1.0):
        self.diffusion = diffusion_model
        self.normalizer = normalizer
        self.action_dim = diffusion_model.action_dim
        self.observation_dim = diffusion_model.observation_dim
        self.goal_weight = goal_weight # ğŸ”¥ã€æ–°å¢ã€‘

    def __call__(self, obs, batch_size=1):
        # 1. å½’ä¸€åŒ–è§‚æµ‹
        obs_in = obs[None, :]
        norm_obs = self.normalizer.normalize(obs_in, 'observations')
        
        # ğŸ”¥ã€å…³é”®ã€‘è¯„ä¼°æ—¶ä¹Ÿè¦ä¹˜æƒé‡ï¼
        if self.goal_weight != 1.0:
            # å‡è®¾ Goal æ˜¯ 7, 8, 9 ç»´ (Dist, Cos, Sin)
            norm_obs[:, 7:10] *= self.goal_weight

        # æˆªæ–­ä¿æŠ¤
        norm_obs = np.clip(norm_obs, -5.0, 5.0)
        
        # 2. æ„é€ æ¡ä»¶
        DEVICE = next(self.diffusion.parameters()).device
        conditions = {0: torch.tensor(norm_obs, device=DEVICE)}
        
        # 3. ç”Ÿæˆè½¨è¿¹
        start_t = time.time()
        with torch.no_grad():
            samples = self.diffusion.conditional_sample(conditions, return_diffusion=False, verbose=False)
            if isinstance(samples, tuple): samples = samples[0]
            samples = samples.cpu().numpy()
        end_t = time.time()
        
        # 4. æå–åŠ¨ä½œ
        norm_action = samples[0, 0, self.observation_dim:] 
        
        # 5. åå½’ä¸€åŒ–
        action = self.normalizer.unnormalize(norm_action[None, :], 'actions')
        action = action[0]

        # è¯Šæ–­æ‰“å°
        if np.random.rand() < 0.05: 
            print(f"\nğŸ” [è¯Šæ–­] Step æ£€æŸ¥:")
            print(f"  > è¾“å…¥è§‚æµ‹ (Norm Min/Max): {norm_obs.min():.3f} / {norm_obs.max():.3f}")
            print(f"  > ç›®æ ‡ä¿¡å· (Obs[7:10] * {self.goal_weight}x): {norm_obs[0, 7:10]}") 
            print(f"  > è¿˜åŸåŠ¨ä½œ (Real): {action}")
            
        return action, samples, (end_t - start_t)

# ================= ä¸»é€»è¾‘ =================
def main():
    # ğŸ”¥ã€æ–°å¢ã€‘å‚æ•°è§£æ
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, default='dataset_raw.npz')
    parser.add_argument('--model_path', type=str, default='./diffuser_checkpoints/default_run', 
                        help='æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„ (ä¾‹å¦‚ ./diffuser_checkpoints/checkpoints_weighted)')
    parser.add_argument('--goal_weight', type=float, default=1.0, 
                        help='è¯„ä¼°æ—¶ä½¿ç”¨çš„ç›®æ ‡æƒé‡ (å¿…é¡»å’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼)')
    parser.add_argument('--num_episodes', type=int, default=5)
    args = parser.parse_args()

    DATASET_PATH = os.path.join(project_root, 'datasets', args.dataset)
    CHECKPOINT_DIR = args.model_path
    DEVICE = 'cuda:0'
    HORIZON = 64

    # 1. åŠ è½½æ•°æ®é›† (Stats)
    print(f"Loading dataset stats: {DATASET_PATH}")
    # æ³¨æ„ï¼šè¿™é‡Œçš„ goal_weight å‚æ•°ç»™ Dataset æ˜¯æ²¡ç”¨çš„ï¼ˆEvalæ—¶ä¸åŠ è½½æ•°æ®ï¼‰ï¼Œä½†ä¸ºäº†ä¿æŒä¸€è‡´æ€§å¯ä»¥ä¼ 
    # å…³é”®æ˜¯åœ¨ Policy.__call__ é‡Œæ‰‹åŠ¨ä¹˜
    dataset = SafetyGymDataset(DATASET_PATH, horizon=HORIZON)
    
    # 2. åŠ è½½æ¨¡å‹ç»“æ„
    model = TemporalUnet(
        horizon=HORIZON,
        transition_dim=26 + 2,
        cond_dim=26,
        dim=256,
        dim_mults=(1, 2, 4)
    ).to(DEVICE)

    diffusion = GaussianDiffusion(
        model=model,
        horizon=HORIZON,
        observation_dim=26,
        action_dim=2,
        n_timesteps=100, 
        loss_type='l2',
        clip_denoised=True,
        predict_epsilon=False,
    ).to(DEVICE)
    diffusion.normalizer = dataset

    # 3. åŠ è½½æƒé‡
    if not os.path.exists(CHECKPOINT_DIR):
         raise FileNotFoundError(f"âŒ ç›®å½•ä¸å­˜åœ¨: {CHECKPOINT_DIR}")
    
    all_files = os.listdir(CHECKPOINT_DIR)
    ckpt_files = [f for f in all_files if f.startswith('state_') and f.endswith('.pt')]
    
    if not ckpt_files:
        raise FileNotFoundError(f"âŒ åœ¨ {CHECKPOINT_DIR} ä¸‹æ²¡æ‰¾åˆ°ä»»ä½• state_*.pt æ¨¡å‹æ–‡ä»¶ï¼")
    
    ckpt_files.sort(key=lambda x: int(x.replace('state_', '').replace('.pt', '')))
    latest_ckpt = ckpt_files[-1]
    
    print(f"ğŸ”„ è‡ªåŠ¨æ£€æµ‹åˆ°æœ€æ–°æ¨¡å‹: {latest_ckpt}")
    ckpt_path = os.path.join(CHECKPOINT_DIR, latest_ckpt)

    print(f"Loading checkpoint: {ckpt_path}")
    state_dict = torch.load(ckpt_path, map_location=DEVICE)
    diffusion.load_state_dict(state_dict['model'])
    diffusion.eval()
    
    # åˆå§‹åŒ–ç­–ç•¥ (ä¼ å…¥ goal_weight)
    print(f"ğŸ”¥ Evaluation Goal Weight: {args.goal_weight}x")
    policy = DiffusionPolicy(diffusion, dataset, goal_weight=args.goal_weight)

    # 4. é¢„çƒ­ GPU
    print("ğŸ”¥ Warming up GPU...")
    dummy_obs = np.zeros(26, dtype=np.float32)
    for _ in range(2): policy(dummy_obs)
    print("âœ… Warmup done.")

    # 5. ç¯å¢ƒ
    print("Creating environment...")
    env = safety_gymnasium.make('SafetyPointGoal1-v0', render_mode='rgb_array', camera_name='fixedfar', width=256, height=256)
    
    # 6. è¯„ä¼°å¾ªç¯
    for ep in range(args.num_episodes):
        print(f"\n=== Episode {ep+1}/{args.num_episodes} ===")
        obs, _ = env.reset()
        done = False
        total_rew = 0
        step = 0
        video_path = f'eval_ep{ep}.mp4'
        writer = imageio.get_writer(video_path, fps=30, macro_block_size=None)
        
        while not done and step < 1000:
            action, _, _ = policy(obs)
            action = np.clip(action, -1.0, 1.0)
            
            # Step
            step_result = env.step(action)
            if len(step_result) == 6:
                 next_obs, reward, cost, terminated, truncated, _ = step_result
            else:
                 next_obs, reward, cost, terminated, truncated = step_result[0], step_result[1], 0, step_result[2], step_result[3]

            try:
                frame = env.render()
                writer.append_data(frame)
            except: pass

            total_rew += reward
            obs = next_obs
            step += 1
            
            if terminated or truncated: done = True
                
        writer.close()
        print(f"Episode Finished. Return: {total_rew:.2f}")

if __name__ == '__main__':
    main()